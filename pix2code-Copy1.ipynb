{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Code\n",
    "\n",
    "Within three years deep learning will change front-end development. It will increase prototyping speed and lower the barrier for building software. The field took off last year when Tony Beltramelli introduced the [pix2code paper](https://arxiv.org/abs/1705.07962) and Airbnb launched [sketch2code](https://airbnb.design/sketching-interfaces/). Currently, the largest barrier to automating front-end development is computing power. However, we can use current deep learning algorithms, along with synthesized training data, to start exploring artificial front-end automation right now!\n",
    "\n",
    "## Turning Design Mockups Into Code With Deep Learning\n",
    "\n",
    "In this notebook, we’ll build a neural network to code a basic a HTML and CSS website based on a picture of a design mockup. \n",
    "\n",
    "![pix2code](https://blog.floydhub.com/content/images/2018/04/bootstrap_overview.gif)\n",
    "\n",
    "*Image from the [Blog](https://blog.floydhub.com/turning-design-mockups-into-code-with-deep-learning/)*\n",
    "\n",
    "\n",
    "We’ll use a dataset of generated bootstrap websites from the [pix2code paper](https://arxiv.org/abs/1705.07962). By using Twitter’s [bootstrap](https://getbootstrap.com/), we can combine HTML and CSS and decrease the size of the vocabulary.\n",
    "\n",
    "Instead of training it on the bootstrap markup, we’ll use 17 simplified tokens that we then translate into HTML and CSS. The [dataset](https://github.com/tonybeltramelli/pix2code/tree/master/datasets) includes 1500 test screenshots and 250 validation images. For each screenshot there are on average 65 tokens, resulting in 96925 training examples.\n",
    "\n",
    "By tweaking the model in the pix2code paper, the model can predict the web components with 97% accuracy ([BLEU](https://en.wikipedia.org/wiki/BLEU) 4-ngram greedy search).\n",
    "\n",
    "We will:\n",
    "\n",
    "- Preprocess webpage images and the code related HTML for this mixed NLP-CV task\n",
    "- Build and train the `pix2code` model using Keras and Tensorflow\n",
    "- Evaluate our model on the test set\n",
    "\n",
    "### Instructions\n",
    "- To execute a code cell, click on the cell and press `Shift + Enter` (shortcut for Run).\n",
    "- To learn more about Workspaces, check out the [Getting Started Notebook]().\n",
    "- **Tip**: *Feel free to try this Notebook with your own data and on your own super awesome pix2code or skecth2code task.*\n",
    "\n",
    "Now, let's get started! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "Let's start by importing some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential, model_from_json\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Embedding, TimeDistributed, RepeatVector, LSTM, concatenate , Input, Reshape, Dense\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Dataset\n",
    "\n",
    "DS_PATH = './datasets/train/' # edit to your /path/to/train/ds\n",
    "DS_EVAL_PATH = './datasets/eval/' # edit to your /path/to/eval/ds\n",
    "# DS_PATH = '../pix2code/datasets/web/all_data/training_set/' # edit to your /path/to/train/ds\n",
    "# DS_EVAL_PATH = '../pix2code/datasets/web/all_data/training_set/' # edit to your /path/to/eval/ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters\n",
    "We'll set the hyperparameters for training our model. If you understand what they mean, feel free to play around - otherwise, we recommend keeping the defaults for your first run 🙂\n",
    "\n",
    "**WARNING**\n",
    "\n",
    "The training is not feasible on a CPU machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    # GPU\n",
    "    EPOCHS = 50 # Number of passes through entire dataset\n",
    "    MAX_SEQUENCE = 150 # The max sequence to predict\n",
    "    MAX_LEN = 48  # Max number of token for the input in the context prediction\n",
    "# Hyperparams for CPU training\n",
    "else:\n",
    "    # CPU\n",
    "    EPOCHS = 50\n",
    "    MAX_SEQUENCE = 100\n",
    "    MAX_LEN = 48\n",
    "    print(\"WARNING: Switch on GPU for training!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "In the step below we will load the imagse and code for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_img(im, figsize=None, ax=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    return ax\n",
    "\n",
    "# Read a file and return a string\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def load_data(data_dir):\n",
    "    text = []\n",
    "    images = []\n",
    "    # Load all the files and order them\n",
    "    all_filenames = listdir(data_dir)\n",
    "    all_filenames.sort()\n",
    "    for filename in (all_filenames):\n",
    "        if filename[-3:] == \"npz\":\n",
    "            # Load the images already prepared in arrays\n",
    "            image = np.load(data_dir+filename)\n",
    "            images.append(image['features'])\n",
    "        else:\n",
    "            # Load the boostrap tokens and rap them in a start and end tag\n",
    "            syntax = '<START> ' + load_doc(data_dir+filename) + ' <END>'\n",
    "            \n",
    "            # Seperate all the words with a single space\n",
    "            syntax = ' '.join(syntax.split())\n",
    "            # Add a space after each comma\n",
    "            syntax = syntax.replace(',', ' ,')\n",
    "            text.append(syntax)\n",
    "    images = np.array(images, dtype=float)\n",
    "    return images, text\n",
    "\n",
    "# Get images and text\n",
    "train_features, texts = load_data(DS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualization\n",
    "# print(\"Here's what an example looks like\\n\")\n",
    "# print(\"HTML bootstrap text:\", texts[0])\n",
    "# ax = show_img(train_features[1], figsize=(4,4))\n",
    "# ax.set_title('HTML bootstrap image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: `btn-orange` represents the *light blue button* and `btn-red` the *blue button*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset info as Sanity check\n",
    "# train_features.shape, len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tokens per sentence plot (see below) is useful for setting the `MAX_LEN` and `MAX_SEQUENCE` training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Custom Tokenizer\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "# Plot sentence by lenght\n",
    "plt.hist([len(tokenize(s)) for s in texts], bins=50)\n",
    "plt.title('Tokens per sentence')\n",
    "plt.xlabel('Len (number of token)')\n",
    "plt.ylabel('# samples')\n",
    "# fig = plt.figure()\n",
    "# fig.patch.set_facecolor('blue')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "For the input data, we will use sentences, starting with the first word and then adding each word one by one. The output data is always one word.\n",
    "Sentences follow the same logic as words. They also need the same input length. Instead of being capped by the vocabulary they are bound by maximum sentence length. If it’s shorter than the maximum length, you fill it up with empty words, a word with just zeros (a.k.a. Padding).\n",
    "\n",
    "![sentence](https://blog.floydhub.com/content/images/2018/04/one_hot_sentence.png)\n",
    "\n",
    "*The image above show one-hot encoding representation for each token, but we will use this representation only for the predictions. Image from the [Blog](https://blog.floydhub.com/turning-design-mockups-into-code-with-deep-learning/).*\n",
    "\n",
    "As you see, words are printed from right to left. This forces each word to change position for each training round. This allows the model to learn the sequence instead of memorizing the position of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the function to create the vocabulary \n",
    "tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
    "# Create the vocabulary \n",
    "tokenizer.fit_on_texts([load_doc('bootstrap.vocab')])\n",
    "\n",
    "# Add one spot for the empty word in the vocabulary \n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "\n",
    "def preprocess_data(texts, features, max_sequence):\n",
    "    X, y, image_data = list(), list(), list()\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    for img_no, seq in enumerate(sequences):\n",
    "        for i in range(1, len(seq)):\n",
    "            # Add the sentence until the current count(i) and add the current count to the output\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # Pad all the input token sentences to max_sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_sequence)[0]\n",
    "            # Turn the output into one-hot encoding\n",
    "            out_seq = to_categorical([out_seq], num_classes=VOCAB_SIZE)[0]\n",
    "            # Add the corresponding image to the boostrap token file\n",
    "            image_data.append(features[img_no])\n",
    "            # Cap the input sentence to MAX_LEN tokens and add it\n",
    "            X.append(in_seq[-MAX_LEN:])\n",
    "            y.append(out_seq)\n",
    "    return np.array(image_data), np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above we are using only 17 tokens (+1 which is for the `PAD`, this ensure that the text will have the same lenght) for encoding the HTML text.  We included the `<START>` and `<END>` tag. These tags are cues for when the network starts its predictions and when to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 1,\n",
       " '{': 2,\n",
       " '}': 3,\n",
       " 'small-title': 4,\n",
       " 'text': 5,\n",
       " 'quadruple': 6,\n",
       " 'row': 7,\n",
       " 'btn-inactive': 8,\n",
       " 'btn-red': 9,\n",
       " 'btn-green': 10,\n",
       " 'btn-orange': 11,\n",
       " 'double': 12,\n",
       " '<START>': 13,\n",
       " 'header': 14,\n",
       " 'btn-active': 15,\n",
       " '<END>': 16,\n",
       " 'single': 17}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Vocabulary\n",
    "tokenizer.word_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data generator, intended to be used in a call to model.fit_generator()\n",
    "# def data_generator(descriptions, features, n_step, max_sequence):\n",
    "#     # loop until we finish training\n",
    "#     while 1:\n",
    "#         # loop over photo identifiers in the dataset\n",
    "#         for i in range(0, len(descriptions), n_step):\n",
    "#             Ximages, XSeq, y = list(), list(),list()\n",
    "#             for j in range(i, min(len(descriptions), i+n_step)):\n",
    "#                 image = features[j]\n",
    "#                 # retrieve text input\n",
    "#                 desc = descriptions[j]\n",
    "#                 # Generate input-output pairs\n",
    "#                 in_img, in_seq, out_word = preprocess_data([desc], [image], max_sequence)\n",
    "#                 for k in range(len(in_img)):\n",
    "#                     Ximages.append(in_img[k])\n",
    "#                     XSeq.append(in_seq[k])\n",
    "#                     y.append(out_word[k])\n",
    "#             # yield this batch of samples to the model\n",
    "#             yield [[array(Ximages), array(XSeq)], array(y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The model is based on Beltramelli‘s [pix2code paper](https://arxiv.org/abs/1705.07962) and Jason Brownlee’s [image caption tutorials](https://machinelearningmastery.com/blog/page/2/).\n",
    "\n",
    "![full](https://blog.floydhub.com/content/images/2018/04/model_more_detail_alone.png)\n",
    "\n",
    "*Image from the [Blog](https://blog.floydhub.com/turning-design-mockups-into-code-with-deep-learning/)*\n",
    "\n",
    "We are learning a function which given an image, predicts one token, then uses the prediction[s] and the image as context for the next precitions until reaching the `<END>` token. \n",
    "\n",
    "In the image below you can see an example where each row is one prediction. To the left are the images represented in their three color channels: red, green and blue and the previous words. Outside of the brackets, are the predictions one by one, ending with a red square to mark the end.\n",
    "\n",
    "![formal](https://blog.floydhub.com/content/images/2018/04/model_function.png)\n",
    "\n",
    "*Image from the [Blog](https://blog.floydhub.com/turning-design-mockups-into-code-with-deep-learning/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create the Image-encoder\n",
    "# image_model = Sequential()\n",
    "# image_model.add(Conv2D(16, (3, 3), padding='valid', activation='relu', input_shape=(256, 256, 3,)))\n",
    "# image_model.add(Conv2D(16, (3,3), activation='relu', padding='same', strides=2))\n",
    "# image_model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "# image_model.add(Conv2D(32, (3,3), activation='relu', padding='same', strides=2))\n",
    "# image_model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "# image_model.add(Conv2D(64, (3,3), activation='relu', padding='same', strides=2))\n",
    "# image_model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "\n",
    "# image_model.add(Flatten())\n",
    "# image_model.add(Dense(1024, activation='relu'))\n",
    "# image_model.add(Dropout(0.3))\n",
    "# image_model.add(Dense(1024, activation='relu'))\n",
    "# image_model.add(Dropout(0.3))\n",
    "\n",
    "# image_model.add(RepeatVector(MAX_LEN))\n",
    "\n",
    "# visual_input = Input(shape=(256, 256, 3,))\n",
    "# encoded_image = image_model(visual_input)\n",
    "\n",
    "# #Create the Text-encoder\n",
    "# language_input = Input(shape=(MAX_LEN,))\n",
    "# language_model = Embedding(VOCAB_SIZE, 50, input_length=MAX_LEN, mask_zero=True)(language_input)\n",
    "# language_model = LSTM(128, return_sequences=True)(language_model)\n",
    "# language_model = LSTM(128, return_sequences=True)(language_model)\n",
    "\n",
    "# #Create the decoder\n",
    "# decoder = concatenate([encoded_image, language_model])\n",
    "# decoder = LSTM(512, return_sequences=True)(decoder)\n",
    "# decoder = LSTM(512, return_sequences=False)(decoder)\n",
    "# decoder = Dense(VOCAB_SIZE, activation='softmax')(decoder)\n",
    "\n",
    "# # Compile the model\n",
    "# model = Model(inputs=[visual_input, language_input], outputs=decoder)\n",
    "# image_model.summary()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save the model for every 2nd epoch\n",
    "# filepath=\"models/org-weights-epoch-{epoch:04d}--loss-{loss:.4f}.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, verbose=1, save_weights_only=True, period=2)\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "# # Optimizer\n",
    "# optimizer = RMSprop(lr=0.0001, clipvalue=1.0)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "If you left the default hyperpameters in the Notebook untouched, your training should take approximately:\n",
    "\n",
    "- On GPU machine: 17-18 hours for 50 epochs.\n",
    "- On CPU: **not working because the model doesn't fit in memory**\n",
    "\n",
    "You can use CPU machine for the model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit_generator(data_generator(texts, train_features, 1, MAX_SEQUENCE), \n",
    "#                     steps_per_epoch=1500, \n",
    "#                     epochs=EPOCHS, \n",
    "#                     callbacks=callbacks_list, \n",
    "#                     verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "It's time to test the trained model. Otherwise you can load the Emil's pretrained model from the dataset as show below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if model: del model\n",
    "\n",
    "# Load model and weights \n",
    "json_file = open('/floyd/input/pix2code/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Load weights into new model\n",
    "model.load_weights(\"/floyd/input/pix2code/weights.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data/anaconda/envs/code/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /data/anaconda/envs/code/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Loaded model from disk\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 48, 19)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 48, 1024)     104098080   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 48, 128)      207360      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 48, 1152)     0           sequential_1[1][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 48, 512)      3409920     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 512)          2099200     lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 19)           9747        lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 109,824,307\n",
      "Trainable params: 109,824,307\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# if model: del model\n",
    "\n",
    "# Load model and weights \n",
    "json_file = open('../pix2code/bin/pix2code.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Load weights into new model\n",
    "model.load_weights(\"../pix2code/bin/pix2code.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat ../pix2code/bin/pix2code.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    photo = np.array([photo])\n",
    "    # seed the generation process\n",
    "    in_text = '<START> '\n",
    "    # iterate over the whole length of the sequence\n",
    "    print('\\nPrediction---->\\n\\n<START> ', end='')\n",
    "    for i in range(150):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = np.argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += word + ' '\n",
    "        # stop if we predict the end of the sequence\n",
    "        print(word + ' ', end='')\n",
    "        if word == '<END>':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, texts = load_data(DS_EVAL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy\n",
    "\n",
    "It’s tricky to find a fair way to measure the accuracy. Say you compare word by word. If your prediction is one word out of sync, you might have 0% accuracy. If you remove one word which syncs the prediction, you might end up with 99/100.\n",
    "\n",
    "Emil used tje BLEU score, best practice in machine translating and image captioning models. It breaks the sentence into four n-grams, from 1-4 word sequences. In the below prediction “cat” is supposed to be “code”.\n",
    "\n",
    "![blue](https://blog.floydhub.com/content/images/2018/04/bleu_score.png)\n",
    "\n",
    "To get the final score you multiply each score with 25%, (4/5) * 0.25 + (2/4) * 0.25 + (1/3) * 0.25 + (0/2) * 0.25 = 0.2 + 0.125 + 0.083 + 0 = 0.408 . The sum is then multiplied with a sentence length penalty. Since the length is correct in our example, it becomes our final score.\n",
    "\n",
    "You could increase the number of n-grams to make it harder. A four n-gram model is the model that best corresponds to human translations. I’d recommend running a few examples with the below code and reading the [wiki page](https://en.wikipedia.org/wiki/BLEU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction---->\n",
      "\n",
      "<START> "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_2 to have 3 dimensions, but got array with shape (1, 48)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2e9f9ac8b874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Eval on the first 10 samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mbleu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BLUE score: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-2e9f9ac8b874>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, descriptions, photos, tokenizer, max_length)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# step over the whole set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphotos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# store actual and predicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\nReal---->\\n\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-a83944213ba5>\u001b[0m in \u001b[0;36mgenerate_desc\u001b[0;34m(model, tokenizer, photo, max_length)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# predict next word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphoto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# convert probability to integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/code/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/code/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/code/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_2 to have 3 dimensions, but got array with shape (1, 48)"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for i in range(len(descriptions)):\n",
    "        yhat = generate_desc(model, tokenizer, photos[i], max_length)\n",
    "        # store actual and predicted\n",
    "        print('\\n\\nReal---->\\n\\n' + texts[i])\n",
    "        actual.append([texts[i].split()])\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    bleu = corpus_bleu(actual, predicted)\n",
    "    return bleu, actual, predicted\n",
    "\n",
    "# Eval on the first 10 samples\n",
    "bleu, actual, predicted = evaluate_model(model, texts[:10], train_features[:10], tokenizer, MAX_LEN)\n",
    "print(\"BLUE score: \", bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiler.classes.Compiler import *\n",
    "\n",
    "#Compile the tokens into HTML and css\n",
    "dsl_path = \"compiler/assets/web-dsl-mapping.json\"\n",
    "compiler = Compiler(dsl_path)\n",
    "compiled_website = compiler.compile(actual[0][0], 'index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HERE'S THE GENERATED HTML\\n\", \"-\"*100, \"\\n\", compiled_website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOWTIME\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(compiled_website))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### That's all folks - don't forget to shutdown your workspace once you're done 🙂"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 Conda (code)",
   "language": "python",
   "name": "conda_code_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
